
We'll focus on designing a web crawler - a classical system design problem. Web crawlers (also called robots) discover new or updated content on the web, such as articles, videos, PDFs, and more.

Common uses include search-engine indexing (e.g., Googlebot), web archiving (preserving content for the future), web mining (extracting domain-specific data), and web monitoring (detecting leaks or copyright infringements).

![web crawler example](https://nextleet.com/images/web-crawler-example.png)

 

## Step 1 - Understand the Problem and Establish Design Scope

At a high level a crawler does three things:

1. Given a set of seed URLs, download the pages those URLs point to.
2. Extract URLs from each downloaded page.
3. Add the newly discovered URLs to the frontier for later traversal.

A production crawler is far more complex - you must clarify the exact feature set with your interviewer before diving into low-level design.

### Example Clarifying Q&A

- **Candidate:** What's the crawler's main purpose?
- **Interviewer:** Search-engine indexing.
- **Candidate:** How many pages per month?
- **Interviewer:** 1 billion pages/month.
- **Candidate:** What content types?
- **Interviewer:** HTML only (for now).
- **Candidate:** Should we detect newly added/edited content?
- **Interviewer:** Yes.
- **Candidate:** Persist crawled pages?
- **Interviewer:** Yes - retain for 5 years.
- **Candidate:** What about duplicate pages?
- **Interviewer:** Ignore duplicates.

Desired characteristics: scalable, robust, polite (rate-limit per host), extensible (support new content types), and efficient at deduplication.

 

## Back-of-the-Envelope Estimation

- 1 billion pages/month approximately 400 pages/sec sustained.
- Assume peak QPS approximately 800 pages/sec.
- Average page size approximately 500 KB - approximately 500 TB/month.
- For 5 years storage approximately 30 PB.

These rough numbers drive choices for storage, network, and processing (parsing, indexing) capacity planning.

 

## Step 2 - High-Level Design and Workflow

A scalable crawler has several components:

- Seed URLs (starting points).
- URL Frontier (priority queue / queues per host).
- HTML Downloader (fetch pages).
- DNS Resolver (cached).
- Content Parser (validate and extract links).
- Content-seen detector (deduplicate by content hash).
- Content Storage (disk / object store with hot cache).
- URL Extractor and URL Filter.
- URL-seen detector (avoid revisiting the same URL).
- URL storage (visited URLs, metadata).

![high level crawler design](https://nextleet.com/images/high-level-design.png)

### Typical Workflow

1. Seed URLs - add to URL Frontier.
2. HTML Downloader pulls URLs from the frontier.
3. DNS Resolver resolves hostnames to IP addresses.
4. Downloader fetches pages; Content Parser validates HTML.
5. Content-seen checks whether page content was already stored (hash-based dedupe).
6. If new, store content; otherwise discard.
7. Extract links from the page.
8. URL Filter excludes invalid/blacklisted content.
9. URL-seen checks whether we've visited the URL before.
10. If not visited, add to URL Frontier.
11. Repeat.

![web crawler workflow](https://nextleet.com/images/web-crawler-workflow.png)

 

## Step 3 - Design Deep Dive

We now dig into key mechanisms: traversal strategy, the frontier, downloader optimizations, robustness, extensibility, and avoiding problematic content.

### DFS vs. BFS

The web is a directed graph. DFS can go very deep (risking spider traps) and typically isn't ideal. BFS (breadth-first) is generally preferable because it discovers pages nearer to seeds first and is easier to prioritize. However, naive BFS can overload a single host because many links from a page point to the same domain, so the frontier must enforce politeness and per-host limits.

### URL Frontier

The frontier manages which URLs to crawl next, enforces politeness, and supports prioritization and freshness.

#### Politeness

Politeness prevents overwhelming hosts. Implementation pattern:

- Maintain one FIFO queue per hostname (or per host:port).
- A global queue router maps hostnames to their queues.
- Enforce a configured delay between successive requests to the same host.
- Queue selector assigns queues to worker threads; workers process from the assigned queue and respect delays.

![download queue per host](https://nextleet.com/images/download-queue.png)

![mapping table host to queue](https://nextleet.com/images/mapping-table.png)

#### Priority

Prioritize URLs using signals such as PageRank estimates, update frequency, or business importance. A prioritizer component assigns priorities and the frontier biases selection toward high-priority queues while preserving politeness.

![prioritizer](https://nextleet.com/images/prioritizer.png)

#### Freshness

Recrawl frequency should be driven by page-change history and importance. Important, frequently-updated pages get shorter recrawl intervals.

#### Frontier Storage

The frontier can be huge. Use a hybrid approach: keep an in-memory buffer for active queues and spill the rest to disk or a fast persistent queue (e.g., RocksDB/leveldb-based queue, or a distributed queue). Periodically refill the in-memory working set.

### HTML Downloader

Downloader fetches pages using HTTP(S). Respect robots.txt and other site directives, and set reasonable timeouts and retry policies.

#### Robots Exclusion Protocol

Respect robots.txt directives. Cache robots.txt per host and revalidate periodically to avoid re-downloading it for every fetch.

```
User-agent: Googlebot
Disallow: /creatorhub/*
Disallow: /rss/people/*/reviews
Disallow: /gp/pdp/rss/*/reviews
Disallow: /gp/cdp/member-reviews/
Disallow: /gp/aw/cr/
```

#### Performance Optimizations

- Distributed crawling: parallelize across many machines; each runs multiple threads to increase throughput.
- DNS cache: maintain an internal DNS cache refreshed periodically to avoid repeated lookups.
- Locality: schedule crawlers geographically close to target servers to reduce latency.
- Short timeouts & backoff: avoid hanging on unresponsive servers.

![distributed crawl](https://nextleet.com/images/distributed-crawl.png)

### Robustness

Robustness techniques include:

- Use consistent hashing for assigning domains/hosts to crawler workers so adding/removing workers moves minimal state.
- Persist crawl state frequently so other workers can pick up after crashes.
- Graceful exception handling and retries; avoid global crashes on local failures.
- Validate and sanitize downloaded HTML to avoid parser crashes.

### Extensibility

Design the crawler modularly: separate download, parse, extract, filter, and storage stages so new content types (images, PDFs) can be added as independent modules.

![extendable crawler](https://nextleet.com/images/extendable-crawler.png)

### Detecting & Avoiding Problematic Content

Watch for:

- Duplicate/redundant content (approximately 30% of pages). Use content hashing to deduplicate.
- Spider traps (infinite loops). Limit URL length/depth, detect repetitive patterns, and allow manual blacklisting.
- Data noise (ads, boilerplate, spam). Apply filters or heuristics to skip low-value pages.

 

## Step 4 - Wrap Up and Additional Considerations

A good crawler is scalable, polite, extensible, and robust. Key design choices are informed by scale, desired freshness, and legal/ethical constraints (robots.txt, rate limits).

### Other Important Talking Points

- Server-side rendering: Many sites use client-side JS; execute or fetch pre-rendered HTML to capture content rendered by JavaScript.
- Filter out unwanted pages early with heuristics and anti-spam components to save resources.
- Storage: shard and replicate crawled content; use object storage for raw HTML and fast indexes for search.
- Keep crawler workers stateless where possible to enable horizontal scaling.
- Instrument heavily: crawl rates, errors, timeouts, robots rejections, duplicate fraction, and data volumes.
- Analytics: collect click/usage data or content-change metrics to refine crawl policies.

When presenting this design in an interview: state assumptions, justify trade-offs (freshness vs cost, politeness vs coverage), and highlight operational aspects (monitoring, rollbacks, staged rollouts).