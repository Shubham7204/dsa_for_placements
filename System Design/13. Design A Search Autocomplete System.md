
Search autocomplete is a feature provided by many platforms such as Amazon and Google. As the user types in the search box, the system returns suggested completions instantly.

The UX expectation is very strict - suggestions should feel instantaneous (ideally <100ms for a smooth experience).

![Google search autocomplete](https://nextleet.com/images/google-search.png)

Ask AI

## Step 1 - Understand the problem and establish design scope

Clarify exact requirements before designing. Example Q&A used to scope the problem:

- **Q:** Is matching only supported at the beginning of a search term or also in the middle?
- **A:** Only at the beginning (prefix matching).
- **Q:** How many suggestions should the system return?
- **A:** 5 suggestions.
- **Q:** Which suggestions should be chosen?
- **A:** Suggestions are ranked by popularity (historical query frequency).
- **Q:** Do we need spell check or auto-correct?
- **A:** No - not in this design.
- **Q:** Are queries in English?
- **A:** Yes; multi-language can be discussed later.
- **Q:** Is capitalization and special character handling required?
- **A:** Assume queries are lowercase characters for simplicity.
- **Q:** How many users?
- **A:** 10 million DAU.

High-level requirements:

- Fast responses (≤ 100ms).
- Relevant results (ranked by popularity).
- Results sorted by frequency/popularity.
- Scalable to many users and high QPS.
- Highly available and fault tolerant.

Ask AI

## Back-of-the-envelope estimation

Quick capacity estimates to guide design decisions.

- 10M DAU × 10 searches/day ≈ 100M searches/day.
- 100M / 86,400 ≈ 1,157 QPS (baseline).
- Assume average search is 4 words × 5 chars each → ~20 keystrokes per search → 1,157 × 20 ≈ 23,140 QPS for keystroke-level autocomplete. Peak QPS ≈ 48,000.
- If 20% of daily queries are new queries: 100M × 0.2 = 20M new queries → at 20 bytes/query ≈ 400 MB/day of new data.

These numbers influence how often we aggregate data, how big our caches must be, and how many query servers are needed.

Ask AI

## Step 2 - High-level design and get buy-in

At a high level the system has two components:

- **Data gathering service** - collects user queries and maintains frequency counts.
- **Query service** - given the typed prefix, returns the top 5 suggestions sorted by popularity.

### Data gathering service

This service ingests raw query logs and updates a frequency table of queries (or prefixes) aggregated over some time window.

![Frequency table example](https://nextleet.com/images/frequency-table.png)

### Query service

The query service looks up the prefix in an index (data structure) and returns the top 5 suggestions. A naive SQL approach works for very small data sets but doesn't scale for high QPS and large vocabularies.

![Query service example](https://nextleet.com/images/query-service-example.png)

![SQL query example](https://nextleet.com/images/query-service-sql-query.png)

Ask AI

## Step 3 - Design deep dive

We explore data structures, aggregation cadence, caching strategies, and scaling.

Ask AI

### Trie data structure

A trie (prefix tree) is a natural fit for prefix searches:

- Each node represents a prefix.
- Traversing to the node for the prefix takes O(p) time, where p is the prefix length.
- Leaf or stored nodes keep candidate completions and their frequencies.

![Trie example with frequency counts](https://nextleet.com/images/trie-example-with-frequency.png)

Basic algorithm: find the node for the prefix (O(p)), traverse its subtree to collect candidate completions (O(c)), and sort candidates by frequency (O(c log c)). In the worst case c can be large, so we use optimizations.

![Trie algorithm steps](https://nextleet.com/images/trie-algorithm.png)

Ask AI

### Optimizations

#### Limit maximum prefix length

Users rarely type extremely long prefixes - capping the prefix length (e.g., 50 characters) bounds the traversal cost and memory per request.

#### Cache top-K completions at each node

Store the top-K (K=5) most popular completions at every trie node. Then a prefix lookup is O(p) and K is constant, so query response is effectively O(p) + O(1). The trade-off is increased memory usage and more expensive updates.

![Caching top search results per trie node](https://nextleet.com/images/caching-top-search-results.png)

Ask AI

### Data gathering service

Instead of updating the trie in real-time for every keystroke (expensive at scale), we collect analytics logs and aggregate them periodically.

![Data gathering architecture](https://nextleet.com/images/data-gathering-service.png)

![Analytics log example](https://nextleet.com/images/analytics-log.png)

Aggregators process raw logs (possibly using streaming platforms like Kafka + stream processors), aggregate frequencies (e.g., hourly or every 30 minutes), and write aggregates to a storage layer. Workers then build the trie (or update a serialized representation) from aggregated data.

Cadence options:

- Near real-time (e.g., 30m) - useful for trending or time-sensitive suggestion updates.
- Daily or weekly - acceptable when freshness is less critical (lower cost).

![Weekly aggregated data example](https://nextleet.com/images/weekly-aggredated-data.png)

Trie persistence options:

- Document store (e.g., MongoDB) - store serialized trie or nodes as documents.
- Key-value store (e.g., DynamoDB, RocksDB) - store nodes or prefix → top-K lists for fast retrieval.

![Trie stored as hashmap](https://nextleet.com/images/trie-as-hashmap.png)

Ask AI

### Query service and caching

The query service first checks an in-memory Trie Cache (hot trie) to serve the top-K for the prefix. On cache miss it falls back to the persistent trie DB.

![Improved query service with cache](https://nextleet.com/images/query-service-improved.png)

Client-side optimizations:

- AJAX requests to avoid full-page refreshes.
- Log a sampled subset of queries to reduce ingestion volume.
- Cache suggestions at the browser for a short TTL (e.g., 1 hour) when suggestions do not change often.

![Google browser caching example](https://nextleet.com/images/google-browser-caching.png)

Ask AI

### Trie operations

#### Create

Workers build the trie from aggregated frequency tables. The built trie (or node shards) is pushed to the Trie Cache and persisted in the Trie DB.

#### Update

Two strategies for updates:

- **Rebuild** the trie periodically from aggregates - simple and consistent; good when real-time changes are not required.
- **Incremental updates** to individual nodes - possible but complex, because when nodes cache top-K lists, updating a leaf may require updating ancestors' cached lists which is costly.

![Update trie illustration](https://nextleet.com/images/update-trie.png)

#### Delete / moderation

To remove hateful or otherwise disallowed suggestions, add a filter layer between the Trie Cache and the Query API. The content store and aggregates should also be updated asynchronously to remove blocked terms.

![Filter layer for moderation](https://nextleet.com/images/filter-layer.png)

Ask AI

### Scaling the storage and sharding

When the trie grows too large for a single machine, shard it. Naive character-range sharding (a-m / n-z) is simple but suffers from uneven distribution (hot shards).

A better approach: use a shard mapper that assigns prefixes to shards based on empirical traffic distribution (frequency histogram). The mapper can dynamically rebalance shards as the distribution changes.

![Sharding trie by prefix ranges](https://nextleet.com/images/sharding.png)

Other considerations: keep hot prefixes (very popular ones) cached on many query servers or in an edge CDN; use consistent hashing for mapping prefixes to shards to minimize reshuffles on resharding.

Ask AI

## Step 4 - Wrap up

We designed an autocomplete system that balances latency, freshness, and cost. Key ideas recap:

- Use a trie (with cached top-K per node) in memory for extremely fast prefix lookups.
- Collect query logs and aggregate them asynchronously; rebuild trie periodically (cadence depends on freshness requirements).
- Serve queries from an in-memory Trie Cache and fall back to persistent trie storage on miss.
- Shard the trie using a smart mapper that accounts for uneven prefix distribution and can rebalance shards.
- Use client-side caching, sampling, and browser caching to reduce load.

### Other talking points

- Multi-language support: store Unicode in trie nodes and build language-specific tries or per-region tries.
- Per-country or per-region tries: build localized tries to return regionally relevant suggestions and use CDNs/edge caches for low latency.
- Real-time trending queries: requires streaming aggregation (Kafka/Storm/Flink) and smaller, frequently-updated trie shards for trending prefixes.
- Ranking improvements: incorporate click-through rates, recency weighting, personalization, or ML models to re-rank suggestions beyond raw frequency.

Final note: the trade-offs are freshness vs. cost and latency vs. complexity. State assumptions and pick a cadence and sharding strategy that match your SLA and budget.