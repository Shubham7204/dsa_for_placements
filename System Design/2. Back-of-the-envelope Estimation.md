In a system design interview, you are sometimes asked to estimate performance requirements or system capacity.

These are usually done with thought experiments and common performance numbers, according to Jeff Dean (Google Senior Fellow).

To make these estimations effectively, there are several key techniques you should know.

 

## Power of Two

Data volumes can become enormous, but the calculations come down to basics.

For accurate calculations, you need to understand powers of two, which correspond to data units:

|Power|Approximate Value|Full Name|Short Name|
|---|---|---|---|
|10|1 Thousand|1 Kilobyte|1 KB|
|20|1 Million|1 Megabyte|1 MB|
|30|1 Billion|1 Gigabyte|1 GB|
|40|1 Trillion|1 Terabyte|1 TB|
|50|1 Quadrillion|1 Petabyte|1 PB|

 

## Latency Numbers Every Programmer Should Know

There is a well-known table of typical computer operation times, created by Jeff Dean.

These numbers may be slightly outdated due to hardware improvements, but they still give a good relative comparison among operations:

|Operation Name|Time|
|---|---|
|L1 cache reference|0.5 ns|
|Branch mispredict|5 ns|
|L2 cache reference|7 ns|
|Mutex lock/unlock|100 ns|
|Main memory reference|100 ns|
|Compress 1 KB (with Zippy)|10,000 ns ≈ 10 µs|
|Send 2 KB over 1 Gbps network|20,000 ns ≈ 20 µs|
|Read 1 MB sequentially from memory|250,000 ns ≈ 250 µs|
|Round trip within same datacenter|500,000 ns ≈ 500 µs|
|Disk seek|10,000,000 ns ≈ 10 ms|
|Read 1 MB sequentially from network|10,000,000 ns ≈ 10 ms|
|Read 1 MB sequentially from disk|30,000,000 ns ≈ 30 ms|
|Send packet California → Netherlands → California|150,000,000 ns ≈ 150 ms|

Here’s a good visualization of the above:

![latency-numbers-visu](https://nextleet.com/images/latency-numbers-visu.png)

Some takeaways from these numbers:

- Memory is fast, disk is slow
- Avoid disk seeks whenever possible
- Compression is usually fast
- Compress data before sending over the network if possible
- Data center round trips are expensive

 

## Availability Numbers

High availability means the ability of a system to stay operational continuously — in other words, minimizing downtime.

Typically, services aim for availability between 99% and 100%.

An SLA (Service Level Agreement) is a formal agreement between a service provider and a customer.

It defines the level of uptime your service must guarantee.

Cloud providers usually set uptime at 99.9% or higher. For example, AWS EC2 has an SLA of 99.99%.

Here’s a summary of allowed downtime based on different SLAs:

![sla-chart](https://nextleet.com/images/sla-chart.png)

 

## Example – Estimate Twitter QPS and Storage Requirements

### Assumptions:

- 300M monthly active users (MAU)
- 50% of users use Twitter daily
- Users post 2 tweets per day on average
- 10% of tweets contain media
- Data is stored for 5 years

### Estimations:

- Write RPS estimation = 150M * 2 / 24h / 60m / 60s ≈ 3400–3600 tweets per second, peak = ~7000 TPS
- Media storage per day = 300M * 10% = 30M media items per day
    - Assume 1MB per media → 30M * 1MB = 30TB per day
    - In 5 years → 30TB * 365 * 5 ≈ 55PB
- Tweet storage estimation:
    - 1 tweet = 64B ID + 140B text + 1000B metadata
    - ~3500 * 60 * 60 * 24 = ~302MB per day
    - In 5 years → 302MB * 365 * 5 ≈ 551GB

 

## Tips

Back-of-the-envelope estimations are about the process, not the exact results. Interviewers often test your problem-solving approach.

Some tips to keep in mind:

- Use rounding and approximation – don’t calculate 99987 ÷ 9.1, round it to 100000 ÷ 10 instead, which is easier.
- Write down your assumptions before starting calculations
- Always label your units clearly. For example, write 5MB instead of just 5.
- Common estimates include: QPS (queries per second), peak QPS, storage, cache size, number of servers.