Digital advertising is a big industry with the rise of platforms such as Facebook, YouTube, TikTok, and others. Hence, tracking ad click events is important. In this chapter, we explore how to design an ad click event aggregation system at Facebook/Google scale.

Digital advertising involves a process called real-time bidding (RTB), where advertising inventory is bought and sold. The speed of RTB is critical - it usually occurs within a second. Data accuracy is also extremely important because it directly affects how much advertisers pay.

Advertisers rely on aggregated ad click events to make decisions such as adjusting target audiences and keywords.

![Digital advertising example](https://nextleet.com/images/digital-advertising-example.png)

Ask AI

## Step 1 - Understand the problem and establish design scope

Clarify requirements and constraints before designing. Below are the key questions and answers that define the scope.

- **Q:** What is the format of the input data?
- **A:** Ad click events with fields like `ad_id`, `click_timestamp`, `user_id`, `ip`, `country`, etc.
- **Q:** What is the expected scale?
- **A:** 1 billion ad clicks per day and about 2 million ads in total. Event volume grows ~30% year-over-year.
- **Q:** What are the important queries the system must support?
- **A:** Typical queries include:

- Return number of click events for ad `X` in the last `Y` minutes.
- Return top 100 most-clicked ads in the past `M` minutes (both parameters configurable; aggregation occurs every minute).
- Support data filtering by `ip`, `user_id`, `country`, etc.

- **Edge cases to consider:**
- Events arriving later than expected (out-of-order / late events).
- Duplicate events.
- Partial outages in parts of the system; need recovery procedures.

**Latency requirement:** A few minutes end-to-end latency for ad-click aggregation is acceptable (these aggregates are used for billing and reporting). For RTB itself, the latency requirement is under a second, but ad-click aggregation can tolerate higher latency.

Ask AI

## Functional and Non-functional requirements

### Functional requirements

- Aggregate the number of clicks for `ad_id` in the last `Y` minutes.
- Return top N most-clicked `ad_id` every minute.
- Support aggregation filtering by attributes such as `ip`, `user_id`, `country`.

### Non-functional requirements

- Correctness: Aggregation must be accurate for billing and RTB decisions.
- Resilience: Handle delayed and duplicate events and tolerate partial failures.
- Latency: End-to-end aggregation latency should be a few minutes at most.
- Scalability: System must handle Facebook/Google-scale data volumes.

Ask AI

## Back-of-the-envelope estimation

Assumptions and quick calculations drive capacity planning:

- 1 billion DAU; assume 1 ad click per user per day -> 1 billion ad clicks per day.
- Average QPS = 1,000,000,000 / 86,400 ≈ 11,574 ≈ 10,000 QPS (rounded).
- Peak QPS assumed to be 5x average -> ≈ 50,000 QPS.
- Assume one ad click event ≈ 0.1 KB stored raw -> daily raw storage ≈ 100 GB; monthly ≈ 3 TB.

Ask AI

## Step 2 - Query API design

The API is the contract between the client (dashboard user: data scientist, analyst, advertiser) and the server. Based on functional requirements, we propose the following endpoints:

### Aggregate number of clicks for an ad

GET /v1/ads/{ad_id}/aggregated_count

Query parameters:

- `from` - start minute (ISO or epoch minute). Default: now - 1 minute.
- `to` - end minute. Default: now.
- `filter` - identifier for filtering strategies (e.g., '001' = non-US clicks).

Response fields:

- `ad_id` - ad identifier.
- `count` - aggregated count between start and end minutes.

### Return top N most-clicked ads

GET /v1/ads/popular_ads

Query parameters:

- `count` - top N most-clicked ads.
- `window` - aggregation window size in minutes.
- `filter` - filtering identifier.

Response: `list` of `ad_id`s ordered by click count.

Ask AI

## Data model

We keep both raw and aggregated data. Raw data enables reprocessing and debugging; aggregated data enables fast queries.

### Raw event schema

|ad_id|click_timestamp|user_id|ip|country|
|---|---|---|---|---|
|ad001|2021-01-01T00:00:01Z|user1|207.148.22.22|USA|
|ad001|2021-01-01T00:00:02Z|user1|207.148.22.22|USA|
|ad002|2021-01-01T00:00:02Z|user2|209.153.56.11|USA|

### Aggregated (per-minute) schema

|ad_id|click_minute|filter_id|count|
|---|---|---|---|
|ad001|202101010000|0012|2|
|ad001|202101010000|0023|3|
|ad001|202101010001|0012|1|
|ad001|202101010001|0023|6|

The `filter_id` represents pre-defined filtering criteria (dimensions):

|filter_id|region|IP|user_id|
|---|---|---|---|
|0012|US|*|*|
|0013|*|123.1.2.3|*|

### Top-N precomputed structure

|window_size|update_time_minute|most_clicked_ads|
|---|---|---|
|integer|timestamp (minute granularity)|JSON array of ad_ids|

Pros and cons of keeping raw vs aggregated data:

- Raw data: full fidelity, supports re-calculation and debugging, but larger storage and slower queries.
- Aggregated data: faster queries and smaller storage for common queries, but is derived and loses fine-grained detail.

Recommended strategy: store raw events (cold/cost-optimised storage) for reprocessing and store per-minute aggregated results in a fast store for queries.

### Database choice considerations

Decide based on: data shape (time-series vs relational), read/write pattern (write-heavy for raw events), transaction needs, and analytical query patterns. For raw events the system is write-heavy (10k QPS average, peaks 50k), so a write-optimised store such as Cassandra or a columnar storage on S3 (Parquet/ORC) is appropriate. Aggregated data is read- and write-heavy; Cassandra or OLAP stores (ClickHouse, Druid) are good choices.

Ask AI

## High-level design

Data flows as an unbounded stream. To decouple producers and consumers and avoid synchronous sinks, use message queues (Apache Kafka) and stream processing for near-real-time aggregation.

![High level design](https://nextleet.com/images/high-level-design-1.png)

We use two message queues:

1. First queue: raw ad click events (`ad_id`, `click_timestamp`, `user_id`, `ip`, `country`).
2. Second queue: aggregated per-minute counts and top-N results (`ad_id`, `click_minute`, `count`; and `update_time_minute`, `most_clicked_ads`).

The second queue helps implement end-to-end exactly-once semantics (atomic commit) between aggregation service and storage.

For aggregation, a MapReduce-like DAG is a clear model: map nodes preprocess and partition events, aggregate nodes count in-memory per-minute, and reduce nodes produce final aggregates. This parallel, distributed computation converts large streams into manageable results.

![Ad count MapReduce](https://nextleet.com/images/ad-count-map-reduce.png)

![Map node example](https://nextleet.com/images/map-node.png)

Ask AI

## Use-cases and pre-aggregation

### Use-case 1 - Aggregate number of clicks

Partition ads (for example by `ad_id % 3`) and aggregate per partition; then reduce to global counts for a time window.

![Use case 1 partitioning](https://nextleet.com/images/use-case-1.png)

### Use-case 2 - Return top N most-clicked ads

Each aggregation node maintains a min-heap of size N for fast top-N retrieval. Reduce phase merges per-node heaps into a global top-N.

![Use case 2 top N](https://nextleet.com/images/use-case-2.png)

### Use-case 3 - Data filtering and dimensions

Predefine filtering criteria (dimensions) and pre-aggregate per dimension using a star schema. Example per-country counts:

|ad_id|click_minute|country|count|
|---|---|---|---|
|ad001|202101010001|USA|100|
|ad001|202101010001|GBP|200|
|ad001|202101010001|others|3000|
|ad002|202101010001|USA|10|

**Note:** The star schema simplifies filtered aggregation queries but increases the number of buckets and stored records as more dimensions are added.

Ask AI

## Step 3 - Design deep dive

Deeper discussion of streaming vs batching, time semantics, windowing, delivery guarantees, deduplication, scaling, fault tolerance, and monitoring.

Ask AI

### Streaming vs. Batching

Our design uses a mixture of streaming (near real-time aggregation) and batching (historical reprocessing and backups). Below is a comparison:

||Services (Online)|Batch (Offline)|Streaming (Near real-time)|
|---|---|---|---|
|Responsiveness|Respond to client quickly|No immediate client response|No immediate client response|
|Input|User requests|Bounded finite datasets|Unbounded streams|
|Output|Responses to clients|Materialized views, aggregates|Materialized views, aggregates|
|Performance metric|Availability, latency|Throughput|Throughput, latency|
|Example|Online shopping|MapReduce jobs|Flink or similar|

Two common architectures are:

- Lambda: separate batch and streaming paths (two codebases; more complexity).
- Kappa: single stream processing path for both real-time and reprocessing (single codebase; simpler operationally).

We prefer a Kappa-like approach so reprocessing of historical data goes through the same aggregation service used for streaming.

![Recalculation example](https://nextleet.com/images/recalculation-example.png)

Ask AI

### Time semantics and watermarks

Timestamps can be taken from event time (when a click occurred) or processing time (when a server processed the event). Due to network and buffering, event time and processing time may differ substantially.

Trade-offs:

|Choice|Pros|Cons|
|---|---|---|
|Event time|More accurate aggregations|Clients may have incorrect clocks or malicious timestamps; must handle late arrivals|
|Processing time|Server-controlled, reliable clock|Late events get assigned to wrong windows, reducing accuracy|

Because accuracy matters for billing, we use **event time** for aggregation and mitigate late events using watermarks and out-of-order handling.

![Watermark technique](https://nextleet.com/images/watermark-technique.png)

Watermark length is a trade-off: short watermarks reduce latency but increase the chance of missed late events; longer watermarks reduce missed events but add latency. End-of-day reconciliation addresses any remaining discrepancies.

Ask AI

### Aggregation windows

Common window types:

- Tumbling (fixed) windows - non-overlapping fixed intervals; good for per-minute counts.
- Hopping windows - fixed size with hop less than size; generates overlapping windows.
- Sliding windows - arbitrary sliding window used for top-N over the last M minutes.
- Session windows - window per user session; not primary for per-ad aggregation.

We use tumbling windows for per-minute counts and sliding windows for top-N in the last M minutes.

![Tumbling window](https://nextleet.com/images/tumbling-window.png)

![Sliding window](https://nextleet.com/images/sliding-window.png)

Ask AI

### Delivery guarantees

Since aggregation results are used for billing, accuracy is critical. Delivery guarantees to consider:

- **At-most-once**: no duplicates, but may miss events.
- **At-least-once**: ensures events are processed but may produce duplicates.
- **Exactly-once**: ideal for billing; requires careful coordination (idempotency, transactional writes, or stream processor support).

For this system, we aim for **exactly-once** semantics or equivalent: deduplicate input events and write aggregates idempotently. Streaming frameworks (Flink, Kafka Streams) and transactional sinks help implement this.

Ask AI

### Data deduplication

Duplicate events arise from clients resending, retries due to outages, or upstream replays. Causes include unacknowledged offsets and retries across components.

![Data duplication example](https://nextleet.com/images/data-duplication-example.png)

Mitigation strategies:

- Idempotent sinks: write aggregates using idempotent operations keyed by a deterministic aggregate key (e.g., `ad_id:minute:filter`), replacing or upserting counts rather than incrementing blindly.
- Deduplication windows: keep a short-lived dedupe-store (Bloom filters or exact keys) to identify duplicates within a window.
- Transactional commit: use transactional writes that atomically advance consumer offsets and persist results.
- End-of-day reconciliation: recompute aggregates from raw events and compare to streaming aggregates to surface discrepancies.

If downstream sinks are idempotent, distributed transactions can be avoided in many cases.

Ask AI

### Scale the system

The system has three decoupled components: message queue, aggregation service (stream processor), and database. Each can be scaled independently.

#### Message queue (Kafka)

- Partition topics adequately in advance to allow parallel consumers.
- Scale consumers by increasing consumer instances within a consumer group.
- Consider geographic partitioning (e.g., topic_na, topic_eu) to localize traffic.
- Plan rebalances off-peak because consumer rebalances can take time at large scale.

Aggregation service scaling

- Scale map/aggregate/reduce nodes horizontally; use multi-threading within nodes for CPU-bound work.
- Use resource managers (YARN, Kubernetes) to allocate extra resources to overloaded tasks.
- Partition by ad_id or use consistent hashing to avoid hotspots; however, extremely popular ads may still create hotspots and require special handling.

![Hotspot issue](https://nextleet.com/images/hotspot-issue.png)

Hotspot mitigation strategies: global-local aggregation (local partial aggregates + global merge), splitting a single hot key into multiple subkeys, or dedicated resources for heavy hitters.

#### Database scaling

- Use horizontally scalable stores (Cassandra, ClickHouse).
- Cassandra supports adding nodes and automatic data rebalancing via consistent hashing.
- For analytical queries, consider OLAP stores (ClickHouse, Druid) or a search layer (Elasticsearch) on top.

Ask AI

### Fault tolerance and recovery

Aggregation nodes keep intermediate in-memory state; node failure can lose this state. Strategies for recovery:

- Use committed consumer offsets in Kafka to resume processing from the last persisted point.
- Periodically snapshot in-memory state (per-minute snapshots) to durable storage so a replacement node can resume from the latest snapshot.
- Keep aggregation results transactional or idempotent so replays do not double-count.
- Have a separate backfill/recalculation pipeline that rebuilds aggregates from raw events when a logic bug or large outage occurs.

![Fault tolerance snapshot](https://nextleet.com/images/fault-tolerance-example.png)

Ask AI

### Data monitoring, correctness and reconciliation

Because aggregated data is used for billing, rigorous monitoring and periodic reconciliation are essential.

Important metrics to track:

- End-to-end latency (event ingestion → aggregated result).
- Message queue lag / unconsumed records.
- Aggregation node resource usage (CPU, memory, GC, disk).
- Error rates, failed writes, and retry counts.

Implement an end-of-day reconciliation job that recomputes aggregates from raw data (batch) and compares them with streaming results. Flag and investigate deviations above a tolerance threshold.

![Reconciliation flow](https://nextleet.com/images/reconciliation-flow.png)

Ask AI

### Alternative design and off-the-shelf tooling

In interviews you are not required to know internals of every big-data tool. However, an alternative practical design uses off-the-shelf tooling:

- Store raw click events in Hive / S3 (Parquet/ORC).
- Use a streaming engine (Flink, Spark Structured Streaming) for near-real-time aggregation.
- Push aggregates to an OLAP store (ClickHouse, Druid) or to an Elasticsearch layer for fast dashboarding and ad-hoc queries.

![Alternative design](https://nextleet.com/images/alternative-design.png)

Aggregation engines commonly used: Apache Flink, Kafka Streams, Spark Streaming. OLAP stores often used for fast analytical queries: ClickHouse, Druid. Choose tools based on operational familiarity, latency requirements, and query patterns.

Ask AI

## Step 4 - Wrap up

Summary of what we covered:

- Data model and API design for per-minute aggregation and top-N queries.
- A Kappa-style architecture using streaming processing with a batch reprocessing path for correctness.
- Handling time semantics and watermarks to manage late events.
- Exactly-once or idempotent aggregation approaches to ensure billing correctness.
- Scaling strategies for message queues, aggregation services, and storage; hotspot mitigation.
- Fault tolerance: snapshots, consumer offsets, and reconciliation/backfill.
- Monitoring, metrics, and end-of-day reconciliation to ensure data correctness.

This design outlines a robust approach to ad click aggregation at very large scale. Familiarity with streaming technologies (Apache Kafka, Apache Flink, Apache Spark) and analytical stores (ClickHouse, Druid) will help implement and operate this system in production.

When presenting this design, state assumptions, explain trade-offs (latency vs. accuracy vs. cost), and emphasize how each component contributes to correctness and scalability.

Ask AI

### References and implementation notes

Relevant technologies and patterns referenced in this chapter:

- Apache Kafka (message queue / commit log)
- Apache Flink (stream processing)
- Apache Spark (batch and streaming)
- Cassandra (wide-column store for high write throughput)
- ClickHouse / Druid (OLAP stores for fast aggregations)

If you are storing this chapter in MongoDB via Mongoose, the document structure matches a schema with a top-level document containing `title`, `chapterNo`, `slug`, and `sections` where each section has `heading`, `slug`, and `content` (HTML string). Ensure the `content` field is stored as HTML (string) and that images referenced by `src` are available at the specified paths.