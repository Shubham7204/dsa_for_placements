The purpose of a rate limiter in a distributed system is to control the rate of traffic sent from clients to a given server.

It controls the maximum number of requests allowed in a given time period. If the number of requests exceeds the threshold, the excess requests are dropped by the rate limiter.

### Examples:

- A user can write no more than 2 posts per second.
- You can create a maximum of 10 accounts per day from the same IP.
- You can claim rewards a maximum of 10 times per week.

Almost all APIs have some sort of rate limiting e.g., Twitter allows a maximum of 300 tweets per 3 hours.

### What are the benefits of using a rate limiter?

- It prevents DoS attacks.
- It reduces costs fewer servers are allocated to lower-priority APIs. Also, you may have downstream dependencies that charge per call, e.g., making a payment or retrieving health records.
- It prevents servers from getting overloaded.



## Step 1 - Understand the Problem and Establish Design Scope

There are multiple techniques you can use to implement a rate limiter, each with its pros and cons.

### Example Candidate–Interviewer Conversation:

- **Candidate**: What kind of rate limiter are we designing? Client-side or server-side?
- **Interviewer**: Server-side.
- **Candidate**: Does the rate limiter throttle API requests based on IP, user ID, or anything else?
- **Interviewer**: The system should be flexible enough to support different throttling rules.
- **Candidate**: What is the scale of the system? Startup or large company?
- **Interviewer**: It should handle a large number of requests.
- **Candidate**: Will the system work in a distributed environment?
- **Interviewer**: Yes.
- **Candidate**: Should it be a separate service or a library?
- **Interviewer**: Up to you.
- **Candidate**: Do we need to inform throttled users?
- **Interviewer**: Yes.

### Summary of Requirements:

- Accurately limit excess requests.
- Low latency and minimal memory usage.
- Support distributed rate limiting.
- Exception handling (whitelists, premium users, etc.).
- High fault tolerance if the cache goes down, the rate limiter should continue functioning.

 

## Step 2 - Propose High-Level Design and Get Buy-in

We will stick with a simple client–server model for simplicity.

### Where to put the rate limiter?

It can be implemented on the client, server, or as middleware.

#### Client-side

Client-side limiting is unreliable because client requests can be forged by malicious actors, and you may not control every client implementation.

#### Server-side

Server-side limiting is reliable and under your control. Example architecture:

![server-side rate limiter](https://nextleet.com/images/server-side-rate-limiter.png)

#### As middleware

A middleware layer between a client and a server can enforce limits before requests reach application servers.

![middleware rate limiter](https://nextleet.com/images/middleware-rate-limiter.png)

An example of how it works when 2 requests per second are allowed:

![middleware rate limiter example](https://nextleet.com/images/middleware-rate-limiter-example.png)

In cloud microservices, rate limiting is commonly implemented in the API gateway, which also handles SSL termination, authentication, IP whitelisting, and static content serving.

Where should you implement it? It depends on:

- Your tech stack some languages/platforms make an in-process library easy.
- If you want fine-grained control over the algorithm, a server-side implementation helps.
- If you already have an API gateway, adding it there may be the fastest option.
- If resources are limited, consider an off-the-shelf third-party solution.

 

## Algorithms for Rate Limiting

Multiple algorithms exist; each has trade-offs: token bucket, leaky bucket, fixed window counter, sliding window log, and sliding window counter.

Choose based on accuracy, memory footprint, burst tolerance, and implementation complexity.

 

### Token Bucket Algorithm

Simple, well understood, and commonly used by large companies (Amazon and Stripe). It works as follows:

- There is a container (bucket) with a predefined capacity.
- Tokens are periodically added to the bucket at a fixed refill rate.
- Once full, no more tokens are added.
- Each request consumes a token.
- If no tokens remain, the request is dropped (or delayed / queued based on policy).

![token bucket algorithm](https://nextleet.com/images/token-bucket-algo.png)

![token bucket algorithm explained](https://nextleet.com/images/token-bucket-algo-explained.png)

Parameters:

- Bucket size the maximum number of tokens allowed.
- Refill rate tokens added per second.

How many buckets do we need? It depends on requirements: per-endpoint, per-IP, per-user, global, or a mix.

#### Pros:

- Easy to implement.
- Memory efficient.
- Supports short bursts if bucket size permits; sustained high traffic is throttled.

#### Cons:

- Tuning parameters (bucket size, refill rate) can be challenging.

 

### Leaking Bucket Algorithm

Similar to the token bucket, but requests are processed at a fixed rate.

- When a request arrives, the system checks if the queue is full. If not, the request is enqueued; otherwise, it is dropped.
- Requests are pulled from the queue and processed at regular intervals (outflow rate).

![leaking bucket algorithm](https://nextleet.com/images/leaking-bucket-algo.png)

Parameters:

- Bucket/queue size how many requests can be held.
- Outflow rate how many requests are processed per interval.

#### Pros:

- Memory efficient.
- Provides a stable outflow rate, useful when downstream systems require a steady load.

#### Cons:

- Bursts fill the queue with older requests recent requests may be dropped or delayed.
- Parameters can be hard to tune.

 

### Fixed Window Counter Algorithm

Time is divided into fixed windows (e.g., 1 minute), and each window has a counter.

- Each request increments the counter for the current window.
- If the counter reaches the threshold, subsequent requests in that window are rejected.

![fixed window counter algorithm](https://nextleet.com/images/fixed-window-counter-algo.png)

A major problem: bursts near window boundaries can allow more requests than intended (edge-case spikes).

Pros: memory efficient, easy to understand, and appropriate when resetting a quota at fixed intervals is desired.

Cons: edge bursts can circumvent limits.

 

### Sliding Window Log Algorithm

This uses a sliding time window and stores timestamps of recent requests (usually in a sorted set, e.g., Redis sorted set).

- When a request arrives, remove outdated timestamps (older than the window).
- Add the timestamp of the new request.
- If the log size is less than or equal to the threshold, allow the request; otherwise, reject it.

![sliding window log algorithm](https://nextleet.com/images/sliding-window-log-algo.png)

#### Pros:

- Very high accuracy.

#### Cons:

- High memory footprint (stores timestamps per key).

 

### Sliding Window Counter Algorithm

A hybrid between fixed window and sliding log: maintains counters for adjacent windows and computes a weighted sum.

![sliding window counter algorithm](https://nextleet.com/images/sliding-window-counter-algo.png)

How it works: keeps counters for each time window; when a request arrives, derive a sliding counter using a weighted average of the current and previous window counts. If it exceeds the threshold, reject the request.

#### Pros:

- Smoother handling of spikes than a fixed window.
- Memory efficient compared to a sliding log.

#### Cons:

- Less accurate than a sliding log; a small fraction (~0.003%) of requests may be inaccurately accepted in experiments.

 

## High-Level Architecture

Use an in-memory cache (like Redis) for storing buckets/counters it's much faster than a disk-backed database for this use case.

![high level architecture](https://nextleet.com/images/high-level-architecture.png)

How it works:

1. A client sends a request to the rate-limiting middleware (or API gateway).
2. The rate limiter fetches the counter/token data from the corresponding bucket in the cache and decides whether to allow the request.
3. If allowed, the request proceeds to the API servers; otherwise, a 429 response is returned or the request is queued.

 

## Step 3 - Design Deep Dive

Questions remaining after the high-level design include: how are rules created and stored? How do we handle rate-limited requests? What is the exact behavior during failures?

 

### Rate Limiting Rules

Rules can be simple or complex (e.g., per-user, per-IP, per-endpoint, or combinations). They are generally stored as configuration files or in a configuration service and are periodically loaded into the in-memory cache by workers.

![lyft rate limiting rules](https://nextleet.com/images/lyft-rate-limiting-rules.png)

![lyft auth rules](https://nextleet.com/images/lyft-rate-limiting-auth-rules.png)

Rules should support versioning, fast rollbacks, and dynamic updates without redeploying services.

 

### Exceeding the Rate Limit

When a request is rate limited, an HTTP 429 (Too Many Requests) is returned.

Optionally, some rate-limited requests can be enqueued for future processing depending on the endpoint's semantics.

#### Helpful HTTP headers to include:

```
X-Ratelimit-Remaining: <number of allowed requests left in window>
X-Ratelimit-Limit: <max calls per time window>
X-Ratelimit-Retry-After: <seconds until the client can retry>
```

 

### Detailed Design

Architecture notes:

- Rules are stored on disk or in a config service; workers periodically populate them into the in-memory cache.
- Rate-limiting middleware intercepts client requests and loads rules from the cache.
- Middleware fetches and updates counters or token-bucket state in Redis.
- If the request is allowed, it proceeds to API servers; otherwise, a 429 is returned, and the request is optionally enqueued.

![detailed design](https://nextleet.com/images/detailed-design.png)

 

### Rate Limiter in a Distributed Environment

Scaling a rate limiter beyond a single server raises synchronization and race-condition challenges.

![race condition](https://nextleet.com/images/race-condition.png)

Typical issues:

- Race conditions when multiple instances simultaneously update the same counter.
- Synchronization across instances without introducing excessive locking.

Locks can solve race conditions but are costly. Better alternatives include Redis atomic operations, Lua scripts, or Redis sorted sets (ZSET) for sliding logs.

![synchronization issue](https://nextleet.com/images/synchronization-issue.png)

If the rate state is kept in process memory, the service becomes stateful and requires sticky sessions to route the same user's requests to the same instance (not ideal).

![redis centralized data store](https://nextleet.com/images/redis-centralized-data-store.png)

Use a centralized, fast datastore (Redis) so rate limiter instances remain stateless. Use atomic operations or Lua scripts to perform check-and-update atomically.

 

### Performance Optimization

- Multi–data-center setup place rate-limiter instances close to users to reduce latency and fail over gracefully between regions.
- Use eventual consistency wisely accept slight over- or under-counting across regions to avoid locking and reduce latency.

Trade-offs: stronger consistency increases correctness but costs latency and complexity.

 

### Monitoring

After deployment, monitor the rate limiter's effectiveness and rules for false positives/negatives.

- Track how many requests are being dropped per rule.
- Track rule hit rates to identify misconfigured rules or abusive clients.
- Monitor latency added by the rate limiter itself.

If too many legitimate requests are dropped, tune rules or algorithm parameters.

 

## Step 4 - Wrap Up

Algorithms we discussed:

- Token bucket good for supporting traffic bursts.
- Leaking bucket good for ensuring a consistent inbound flow to downstream services.
- Fixed window good when quotas are reset at explicit windows.
- Sliding window log best accuracy at the expense of memory.
- Sliding window counter balanced accuracy with a low memory footprint.

### Additional Talking Points (if time permits):

- Hard vs. soft rate limiting: hard = cannot exceed a threshold; soft = can exceed a threshold briefly.
- Rate limiting at different layers L7 (application) vs. L3 (network).
- Client-side measures to avoid being rate limited: caching, understanding limits, back-off, and retry logic.

Design decisions depend on requirements: consistency vs. latency, memory vs. accuracy, and operational complexity. Always state assumptions, measure post-deployment, and iterate.