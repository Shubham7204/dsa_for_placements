In this chapter we'll design a distributed message queue.

Benefits of message queues:

- **Decoupling** - Eliminates tight coupling between components so they can be developed and deployed independently.
- **Improved scalability** - Producers and consumers can be scaled independently based on traffic.
- **Increased availability** - If one part of the system goes down, other parts can continue interacting via the queue.
- **Better performance** - Producers can emit messages without waiting for consumers to finish processing.

Some popular implementations: Kafka, RabbitMQ, RocketMQ, Apache Pulsar, ActiveMQ, ZeroMQ. Strictly speaking, Kafka and Pulsar are event streaming platforms; nevertheless, feature convergence blurs the distinction.

In this chapter we build a message queue supporting advanced features such as long data retention and repeated message consumption.

Ask AI

## Step 1 - Understand the problem and establish design scope

Message queues must handle producers producing messages and consumers consuming them. Design decisions vary based on persistence, ordering, delivery semantics, retention, and throughput.

Representative Q&A between Candidate and Interviewer:

- **Q:** What's the format and average message size? Are messages text only?
- **A:** Messages are text-only and usually a few KB.
- **Q:** Can messages be consumed repeatedly?
- **A:** Yes - messages can be consumed repeatedly by different consumers. This requirement goes beyond traditional message queues.
- **Q:** Are messages consumed in the same order they were produced?
- **A:** Yes - ordering must be preserved. This is an additional requirement compared to many traditional queues.
- **Q:** What are the data retention requirements?
- **A:** Messages must be retained for two weeks.
- **Q:** How many producers and consumers should we support?
- **A:** As many as possible; design for large scale.
- **Q:** Which delivery semantics are required? At-most-once, at-least-once, exactly-once?
- **A:** We must support at-least-once; ideally configure all semantics.
- **Q:** What's the throughput / latency target?
- **A:** Support both high-throughput (e.g., log aggregation) and low-latency use cases; configuration/tuning should allow trade-offs.

### Functional requirements

- Producers send messages to a queue.
- Consumers consume messages from the queue.
- Messages can be consumed once or repeatedly.
- Historical data can be truncated per retention policy.
- Message size is typically in the KB range.
- Order of messages must be preserved (within a scope / partition).
- Delivery semantics configurable: at-most-once / at-least-once / exactly-once.

### Non-functional requirements

- High throughput or low latency depending on tuning.
- Scalable and distributed; handle sudden surges.
- Persistent and durable storage with replication.

Note: Traditional message queues often don't support long retention and strict ordering at large scale; those constraints increase design complexity.

Ask AI

## Step 2 - High-level design and get buy-in

Key components of a message queue:

![Message queue components](https://nextleet.com/images/message-queue-components.png)

- **Producer** - client that sends messages to a topic/queue.
- **Consumer** - client that subscribes and consumes messages.
- **Broker** - servers that host partitions and store messages.
- **Data storage** - persistent on-disk storage for messages.
- **State storage** - stores consumer offsets, group state.
- **Metadata storage** - stores topic configuration and cluster metadata.
- **Coordination service** - service discovery and leader election (e.g., ZooKeeper/etcd).

Ask AI

## Messaging models

### Point-to-point

A message is sent to a queue and consumed by exactly one consumer. Multiple consumers can exist, but each message is delivered to only one consumer and removed once acknowledged. Traditional point-to-point queues typically do not provide long retention.

![Point to point model](https://nextleet.com/images/point-to-point-model.png)

### Publish-subscribe (Pub/Sub)

Messages are published to a topic. Consumers subscribe to that topic and receive all messages. Event streaming platforms lean toward pub/sub semantics with retention and repeatable consumption.

![Publish subscribe model](https://nextleet.com/images/publish-subscribe-model.png)

Ask AI

## Topics, partitions and brokers

To scale a high-volume topic, split it into partitions (shards). Partitions are hosted by brokers. Ordering is guaranteed within a partition.

![Partitions](https://nextleet.com/images/partitions.png)

- Messages for a topic are distributed across partitions.
- Each partition provides FIFO ordering.
- **Offset**: the position of a message within a partition.
- A partition key (e.g., `user_id`) controls which partition a message lands in, enabling ordering per key.

Ask AI

## Consumer groups

A consumer group is a set of consumers that jointly consume messages from a topic. Each consumer group maintains its own offsets; messages are replicated per group (not per consumer).

![Consumer groups](https://nextleet.com/images/consumer-groups.png)

Parallel consumption by a consumer group improves throughput but affects ordering: at most one consumer in a group should consume a specific partition to preserve ordering. Therefore, the number of active consumers in a group is typically limited by partition count.

Ask AI

## High-level architecture

High-level components and relationships:

![High level architecture](https://nextleet.com/images/high-level-architecture.png)

- Producers push messages to topics.
- Brokers host partitions and persist messages.
- State storage keeps consumer offsets and group assignments.
- Metadata storage holds topic configuration and replica distribution.
- Coordination service handles leader election and discovery.

Ask AI

## Step 3 - Design deep dive

To meet high throughput and long retention, we make several choices: use on-disk append-friendly structures, immutable message records, and batching.

Ask AI

### Data storage

Message storage characteristics:

- Write-heavy and read-heavy depending on retention and consumer patterns.
- No updates - messages are appended; deletions only via retention policies.
- Predominantly sequential read/write patterns.

Options considered:

- Traditional databases are not ideal for very high sequential append throughput.
- Write-Ahead Log (WAL) style append-only files are ideal - the data is split into segments so files never grow excessively large. Old segments become read-only; only the latest segment is written to.

![WAL example](https://nextleet.com/images/wal-example.png)

Sequential access patterns leverage HDD and OS page cache effectively - sequential throughput is high even on rotating disks. OS-level caching further improves performance.

Ask AI

### Message data structure

Keep the message schema consistent between producer, broker, and consumer to avoid extra copying and serialization overhead.

![Message structure](https://nextleet.com/images/message-structure.png)

- **Key** - used for partitioning: `hash(key) % numPartitions`. Keys need not be unique.
- **Value** - payload (text or compressed binary).
- **Topic** - logical channel.
- **Partition** - numeric partition id.
- **Offset** - position within partition.
- **Timestamp** - when stored or event time.
- **Size** - message size.
- **CRC** - checksum for integrity.

Additional fields (eg tags) can support filtering or routing.

Ask AI

### Batching

Batching in producer, broker, and consumer is critical for throughput because it amortizes network and disk IO costs and creates large sequential writes to storage.

Trade-off between latency and throughput:

![Batch size tradeoff](https://nextleet.com/images/batch-size-throughput-vs-latency.png)

Tune batch sizes based on use-case: small batches for low-latency scenarios, larger batches for high-throughput scenarios. Increasing partitions can help throughput if sequential disk write becomes a bottleneck.

Ask AI

### Producer flow

Which broker should a producer send a message to? Two approaches:

#### Routing layer

A separate routing service reads the replica plan from metadata, caches it, and forwards messages to the leader replica for the chosen partition. Follower replicas pull from the leader. Drawbacks: extra network hop and harder batching.

![Routing layer](https://nextleet.com/images/routing-layer.png)

#### Embedded routing in producer

Embed partition mapping and routing logic in the producer so it connects directly to the leader. Benefits: fewer network hops, producers control partition selection, and can batch messages in-memory for larger requests.

![Routing embedded in producer](https://nextleet.com/images/routing-layer-producer.png)

Batch size selection is the standard latency vs throughput trade-off.

Ask AI

### Consumer flow

Consumers specify an offset and fetch messages starting from it. Two consumption models: push and pull.

#### Push vs Pull

Push model: broker pushes messages to consumers as they arrive. Lower latency but risk of overwhelming slower consumers; broker must manage flow control.

Pull model: consumers request messages, controlling consumption rate. Better for batch processing and handling consumers with varying capacities; higher latency and extra network calls which can be mitigated with long polling.

![Consumer flow](https://nextleet.com/images/consumer-flow.png)

Typical flow for consumer group join:

1. Consumer subscribes to topic and joins a group.
2. A group coordinator (found by hashing group name) assigns partitions to consumers.
3. Consumers fetch messages starting from their assigned offsets stored in state storage.
4. After processing, consumer commits offsets to the broker/state store. The order of processing vs committing determines delivery semantics.

Ask AI

### Consumer rebalancing

Rebalancing assigns partitions among group members when consumers join/leave or partitions change. A coordinator orchestrates rebalances.

![Consumer rebalancing](https://nextleet.com/images/consumer-rebalancing.png)

Protocol outline:

1. Consumers send heartbeats to the coordinator.
2. Coordinator detects membership changes or partition changes and triggers a rebalance.
3. Coordinator selects a leader among consumers; leader computes partition assignment and sends plan back to coordinator.
4. Coordinator broadcasts assignment; consumers start consuming assigned partitions from stored offsets.

Joining, leaving, or heartbeat timeouts all trigger similar rebalancing workflows.

Ask AI

### State storage

State storage holds partition-to-consumer mappings and last consumed offsets per group and partition.

Access patterns:

- Frequent reads/writes but low volume.
- Updates happen often; deletes are rare.
- Random read/write access.
- Strong consistency is important.

A fast, consistent key-value store such as ZooKeeper or a dedicated coordinator service is a good fit.

![State storage](https://nextleet.com/images/state-storage.png)

Ask AI

### Metadata storage

Metadata stores topic properties: partition count, retention policy, replica distribution, ACLs, etc. Metadata changes infrequently but consistency is critical.

ZooKeeper (or etcd) is commonly used for metadata because it provides strong consistency and primitives for leader election and watches.

Ask AI

### ZooKeeper

ZooKeeper is a hierarchical consistent key-value store commonly used for distributed coordination: service discovery, configuration, leader election, and small metadata/state storage.

![ZooKeeper](https://nextleet.com/images/zookeeper.png)

In our design, brokers store messages while ZooKeeper manages metadata and coordination tasks (leader election, partition assignment plans, etc.).

Ask AI

### Replication

To tolerate hardware failures, partitions are replicated across brokers. For each partition, one replica is the leader and others are followers. Producers send to the leader; followers pull from the leader.

![Replication example](https://nextleet.com/images/replication-example.png)

Leader decides replica distribution and writes the plan to metadata storage (ZooKeeper). Replication ensures availability and durability.

Ask AI

### In-sync replicas (ISR)

ISR is the set of replicas kept up to date with the leader. A replica falls out of ISR if it lags beyond configured thresholds (e.g., `replica.lag.max.messages`).

![In sync replicas](https://nextleet.com/images/in-sync-replicas-example.png)

Commit semantics depend on acknowledgments:

- **ACK=all** - leader waits for all ISR replicas to persist message before acknowledging; highest durability, higher latency.
- **ACK=1** - leader acknowledges once it persists; moderate durability, lower latency.
- **ACK=0** - leader does not wait; lowest durability, lowest latency.

ISR size and ack policy are trade-offs between latency and safety.

Ask AI

### Consumer read strategy and leader load

Simplest design: all consumers read from the partition leader. This centralizes reads and simplifies coordination. It works as long as the number of connections remains manageable.

For very hot topics, increase partition count or consider letting a consumer read from a follower replica (read from ISR) in special scenarios, such as geo-local consumption. Extra care is needed to ensure consistency.

Ask AI

### Scalability

How different parts scale:

#### Producer

Producers are lightweight; scale by adding instances. Embedded routing and batching minimize network overhead.

#### Consumer

Consumer groups provide isolation; add consumers to increase parallelism. Rebalancing handles membership changes.

#### Broker

Brokers scale by adding more nodes and redistributing partition replicas. On broker failure, leader election and reassignment maintain availability.

![Broker failure recovery](https://nextleet.com/images/broker-failure-recovery.png)

Key considerations: distribute replicas across brokers (and datacenters if needed) to avoid correlated failure. Rebalancing and replica redistribution strategies must be carefully planned.

#### Partition

Adding partitions increases parallelism. When decreasing partitions, decommissioning is gradual: stop sending new messages to decommissioned partitions, allow consumers to drain them, and truncate after retention expires.

![Partition example](https://nextleet.com/images/partition-exmaple.png)

Ask AI

### Data delivery semantics

Common guarantees:

#### At-most-once

Messages are delivered at most once and may be lost. Producer sends asynchronously without retries; consumer commits offsets immediately on fetch. If consumer crashes before processing, the message may be lost.

![At most once](https://nextleet.com/images/at-most-once.png)

#### At-least-once

Messages are delivered at least once; duplicates may occur. Producer retries until ack; consumer commits offset only after processing. If consumer crashes after processing but before committing, duplication happens.

![At least once](https://nextleet.com/images/at-least-once.png)

#### Exactly-once

Exactly-once semantics are the most user-friendly but costly: require transactional writes, idempotent operations, or stream processors with transactional sinks to achieve end-to-end exactly-once.

![Exactly once](https://nextleet.com/images/exactly-once.png)

Ask AI

### Advanced features

#### Message filtering

Some consumers want only a subset of messages. Options:

- Create separate topics per subset - simple but costly (storage duplication and tight coupling).
- Client-side filtering - simple, but wastes network and broker IO.
- Broker-side filtering using message tags. Consumers subscribe to tags; broker filters before sending. Safer than parsing payloads and avoids data transfer for irrelevant messages.
- Complex filtering (predicate or script execution) is possible but computationally expensive and may hurt broker performance.

![Message filtering](https://nextleet.com/images/message-filtering.png)

#### Delayed & scheduled messages

Use cases sometimes require scheduling or delaying messages (e.g., retry a payment check after 30 minutes). Techniques:

- Temporary storage (delay topic) and a scheduler that moves messages to the target partition when the delay expires.
- Hierarchical timing wheel or dedicated delay queues to efficiently schedule and expire messages.

![Delayed messages implementation](https://nextleet.com/images/delayed-message-implementation.png)

Ask AI

## Step 4 - Wrap up

Additional talking points to consider when implementing or presenting this design:

- **Communication protocol:** choose a protocol that supports your use-cases and high volume (e.g., Kafka protocol, AMQP). Ensure message integrity (checksums, versioning).
- **Retry consumption:** messages that cannot be processed immediately can be sent to dedicated retry topics or dead-letter queues to be retried later.
- **Historical archive:** old messages can be moved to archive storage (HDFS, S3) to save space while keeping the ability to reprocess.
- **Monitoring and metrics:** track latency, consumer lag, ISR membership, broker resource usage, and error rates.
- **Operational tooling:** provide admin APIs for partition management, replica reassignment, and rebalancing observation.

The design outlined supports durability, ordering (within partitions), configurable delivery semantics, long retention, and advanced features such as filtering and scheduled delivery. Tuning (batch sizes, partition count, replication/ack policies) lets the system fit varying workloads from low-latency queues to high-throughput streaming.