Key-value stores are a type of non-relational database where each unique identifier (the _key_) maps to an associated _value_. They are intentionally simple and extremely fast for many use cases.

Characteristics:

- Each unique identifier is stored as a key with a value associated with it.
- Keys must be unique and can be plain text or hashes.
- Performance-wise, short keys usually work better.

### Example keys:

- Plain-text: `"last_logged_in_at"`
- Hashed key: `253DDEC4`

We'll design a key-value store that supports two basic operations:

```
put(key, value)  // insert or update value associated with key
get(key)         // retrieve value associated with key
```

 

## Understand the problem and establish design scope

When designing a key-value store, there are several trade-offs to balance: read vs write throughput, memory usage, and consistency vs availability. Be explicit about which guarantees you want because they affect architecture choices.

Target characteristics for our design:

- Key-value pair size is small around `10KB`.
- Able to store very large datasets spanning many nodes.
- High availability system must respond quickly, even during some failures.
- High scalability scale horizontally to support growth.
- Automatic scaling adding/removing servers should be easy and automated.
- Tunable consistency operators should be able to trade some consistency for latency when needed.
- Low latency for reads and writes.

 

## Single server key-value store

On a single machine, the simplest implementation is an in-memory hash map (dictionary) that stores key-value pairs. This is trivial and fast but limited by the machine's memory.

Scaling limits and mitigations:

- Memory is a bottleneck; consider data compression to reduce footprint.
- Store only frequently-accessed data in memory (hot data) and keep the rest on disk.

However, even with compression and caching, a single server cannot hold massive datasets or provide the availability guarantees we want, which motivates a distributed design.

 

## Distributed key-value store

A distributed key-value store distributes data across many nodes using a distributed hash table (DHT) so that keys are partitioned across nodes. This enables horizontal scaling and higher availability when implemented correctly.

When designing a distributed store, you must consider the **CAP theorem**:

![CAP theorem](https://nextleet.com/images/cap-theorem.png)

### CAP theorem summary

- Consistency: All clients see the same data at the same time.
- Availability: Every request receives a response, regardless of which node it reaches.
- Partition tolerance: The system continues to operate despite network partitions.

Because network partitions are inevitable in real distributed systems, you must choose between consistency and availability during partitions. Different systems make different trade-offs (e.g., Dynamo-style stores often choose availability with eventual consistency).

 

## System components

Key components to consider when building a distributed key-value store:

1. Data partitioning
2. Data replication
3. Consistency protocol and versioning
4. Failure detection and recovery
5. Local storage (commit log, memtable, SSTables)

 

### Data partition

For large datasets, you must partition (shard) data across many nodes. A common approach is consistent hashing:

![consistent hashing](https://nextleet.com/images/consistent-hashing.png)

Advantages of consistent hashing:

- Automatic scaling adding/removing servers minimally affects key locations.
- Heterogeneity higher-capacity machines can be assigned more virtual nodes.

 

### Data replication

To achieve availability and durability, replicate each key across multiple nodes (replicas). On the hash ring, choose successive nodes clockwise as replica targets.

![data replication across nodes](https://nextleet.com/images/data-replication.png)

Caveat: ensure that replicas for a single key are placed on distinct physical machines (avoid selecting multiple vnodes on the same physical host) and, when possible, on separate data centers to tolerate datacenter-level failures.

 

### Consistency and quorum

When data is replicated, you must define read and write quorums to control consistency:

- N total number of replicas for each key.
- W number of replicas that must acknowledge a write to consider it successful.
- R number of replicas that must respond to a read.

Trade-offs:

- `W = 1, R = 1` low latency, eventual consistency.
- `W + R > N` provides strong consistency but increases latency.
- Common patterns: `R = 1, W = N` (fast reads, slow writes) or `R = N, W = 1` (fast writes, slow reads).

 

### Consistency models

- Strong consistency reads return the most up-to-date value; clients never see stale data.
- Weak consistency reads might not reflect recent writes.
- Eventual consistency reads might be stale, but updates converge over time to the latest state.

Strong consistency often requires blocking operations during partitions; eventual consistency (used by Dynamo and Cassandra) favors availability while requiring conflict resolution mechanisms.

 

### Versioning and inconsistency resolution

Replication can lead to conflicting versions of the same key. One way to detect and resolve conflicts is vector clocks.

A vector clock is a set of `[server_id, counter]` pairs attached to each object. Each time a replica modifies the object, it increments its counter. By comparing vector clocks you can see whether one version is an ancestor of another or whether versions have diverged (conflict).

Example workflow:

1. Client writes D1 handled by Sx â†’ D1 gets version `[Sx:1]`.
2. Later, Sx updates to `[Sx:2]`.
3. Another client updates from a different replica Sy producing `[Sx:2, Sy:1]`.
4. Simultaneous writes on Sz could produce `[Sx:2, Sz:1]`, which is concurrent with the previous version.
5. When a client reads two concurrent versions, it must reconcile them (application-specific merge) and write back a merged version with an updated vector clock.

Trade-offs: vector clocks increase client complexity and can grow in size (many replicas); they are suitable when application-level conflict resolution is possible.

 

### Handling failures

Failures are inevitable; the system must detect and recover from them. Strategies include failure detection, temporary failure handling, and permanent failure recovery.

 

#### Failure detection

Simple failure detection by one host failing to respond is insufficient. Decentralized techniques like gossip protocols are efficient at scale.

Gossip protocol summary:

- Each node maintains a membership list with heartbeat counters.
- Nodes periodically increment their heartbeats and send them to a random subset of other nodes.
- Receivers update and propagate heartbeats, so information spreads rapidly and reliably.
- If a node's heartbeat isn't updated within a timeout, it is suspected offline; the suspicion is gossip-propagated and confirmed by other nodes.

![gossip protocol](https://nextleet.com/images/gossip-protocol.png)

 

#### Handling temporary failures (hinted handoff)

When a primary replica is temporarily down, another healthy node can accept writes on its behalf and store them with a hint that the primary should receive this data later. This is called hinted handoff. Once the primary returns, the hint is handed off and the primary is reconciled.

Hinted handoff improves availability during transient outages without requiring immediate full replication.

 

#### Handling permanent failures (anti-entropy)

For permanent or long-lived replica divergence, anti-entropy mechanisms reconcile replicas. Merkle trees are commonly used to compare large datasets efficiently.

A Merkle tree is a hash tree where leaves are hashes of data buckets and internal nodes hash their children. Two replicas compare root hashes; if they differ, they recursively compare children, allowing them to exchange only differing buckets instead of entire datasets.

![merkle tree](https://nextleet.com/images/merkle-tree.png)

 

#### Handling data center outage

To survive a data center outage, replicate data across multiple geographical regions (multi-datacenter replication). This increases durability but introduces additional complexity (cross-region latency, consistency decisions, and cost).

Carefully design replication strategies and failure domains so that a single data center failure does not make the data unavailable.

 

### System architecture diagram

High-level architecture:

![system architecture diagram](https://nextleet.com/images/architecture-diagram.png)

Main features:

- Clients communicate with the key-value store through a simple API or coordinator/proxy.
- A coordinator/proxy routes requests to the appropriate nodes based on consistent hashing.
- Nodes are placed on a hash ring using consistent hashing.
- The system is decentralized; adding and removing nodes is supported and can be automated.
- Data is replicated across multiple nodes and possibly across multiple data centers; there is no single point of failure.

![node responsibilities](https://nextleet.com/images/node-responsibilities.png)

 

### Write path

Typical write path (Log-structured storage approach):

1. Write requests are appended to a durable commit log (write-ahead log) on disk to ensure durability.
2. Data is saved in an in-memory structure (memtable) for fast reads and writes.
3. When the in-memory table reaches a threshold, its contents are flushed to disk as an SSTable (Sorted String Table).

SSTables are immutable, sorted files on disk that make range scans and merges efficient. Compaction merges SSTables to reduce lookup overhead and reclaim space.

![write path](https://nextleet.com/images/write-pth.png)

 

### Read path

Read behaviour depends on data location:

#### If data is in memory:

Fetch from the in-memory structure (memtable) and return to the client this is the fastest path.

![read path in memory](https://nextleet.com/images/read-path-in-memory.png)

#### If data is not in memory:

Check SSTables on disk. Use a bloom filter to quickly test whether an SSTable may contain the key and avoid unnecessary disk seeks. Merge results from memtable and SSTables to find the most recent value.

![read path not in memory](https://nextleet.com/images/read-path-not-in-memory.png)

 

### Summary

We covered major design choices and techniques for building a scalable, available key-value store. Below is a concise mapping of goals to common techniques.

|Goal / Problem|Technique|
|---|---|
|Ability to store big data|Use consistent hashing to spread load across servers|
|High availability reads|Data replication, multi-datacenter setup|
|Highly available writes|Versioning and conflict resolution with vector clocks|
|Dataset partition|Consistent hashing|
|Incremental scalability|Consistent hashing and vnode assignments|
|Heterogeneity|Assign more virtual nodes to higher-capacity machines|
|Tunable consistency|Quorum consensus (N, R, W)|
|Handling temporary failures|Sloppy quorum and hinted handoff|
|Handling permanent failures|Merkle trees for efficient anti-entropy|
|Handling datacenter outage|Cross-datacenter replication and careful failure domains|

Design notes: clearly state your consistency and availability requirements, tune replication and quorum sizes accordingly, and instrument the system heavily for monitoring and alerting so you can iterate after deployment.