Payment platforms usually have a wallet service, where they allow clients to store funds within the application, which they can withdraw later.

You can also use it to pay for goods & services or transfer money to other users who use the digital wallet service. That can be faster and cheaper than doing it via normal payment rails.

![digital-wallet](https://nextleet.com/images/digital-wallet.png)

Ask AI

## Step 1 - Understand the Problem and Establish Design Scope

Clarify scope with key questions:

- **C:** Should we only focus on transfers between digital wallets? Should we support any other operations?
- **I:** Let's focus on transfers between digital wallets for now.
- **C:** How many transactions per second does the system need to support?
- **I:** Let's assume 1 million TPS.
- **C:** A digital wallet has strict correctness requirements. Can we assume transactional guarantees are sufficient?
- **I:** Sounds good.
- **C:** Do we need to prove correctness?
- **I:** We can do that via reconciliation, but that only detects discrepancies vs. showing us the root cause for them. Instead, we want to be able to replay data from the beginning to reconstruct the history.
- **C:** Can we assume availability requirement is 99.99%?
- **I:** Yes.
- **C:** Do we need to take foreign exchange into consideration?
- **I:** No, it's out of scope.

In summary we must: support balance transfers between two accounts, support 1M TPS, ensure 99.99% reliability, provide transactional guarantees and reproducibility.

Ask AI

### Back-of-the-envelope estimation

A traditional relational database in the cloud can handle ~1k TPS. To reach 1M TPS you'd need many nodes or approaches to raise per-node throughput. Considering two legs per transfer, the effective requirement is ~2M small operations per second.

|Per-node TPS|Node Number|
|---|---|
|100|20,000|
|1,000|2,000|
|10,000|200|

Ask AI

## Step 2 - Propose High-Level Design and Get Buy-In

We'll propose APIs, a sharding approach, and transaction models (2PC, TCC, Saga), and discuss event sourcing for reproducibility.

Ask AI

### API Design

We only need one primary endpoint for the interview:

```
POST /v1/wallet/balance_transfer
{
  "from_account": "A123",
  "to_account": "B987",
  "amount": "1.00",
  "currency": "USD",
  "transaction_id": "idempotency-uuid"
}
```

Sample response:

```
{
  "status": "success",
  "transaction_id": "01589980-2664-11ec-9621-0242ac130002"
}
```

Notes: `amount` is a string (avoid floating point), `transaction_id` acts as an idempotency key. The endpoint must be highly available and authenticated.

Ask AI

### In-memory sharding solution

A natural first design: keep balances in-memory (Redis) as `map`. Because one Redis node cannot handle 1M TPS, partition data across many Redis nodes using consistent hashing or fixed hash partitioning.

Example partitioning algorithm:

```
String accountID = "A";
Int partitionNumber = 7;
Int myPartition = accountID.hashCode() % partitionNumber;
```

Zookeeper (or similar) can store partition count and addresses. The wallet service is stateless and routes operations to the correct partition.

This approach is scalable but doesn't provide cross-partition atomic transfers - we must address atomicity for debit+credit legs.

![wallet-service](https://nextleet.com/images/wallet-service.png)

Ask AI

### Distributed transactions (2PC)

One way to get atomic multi-shard transfers is 2PC across transactional databases. A coordinator asks participants to prepare, then commit if all agree.

![distributed-transactions-relational-dbs](https://nextleet.com/images/distributed-transactions-relational-dbs.png)

Downsides: lock contention, performance hit, coordinator single point of failure. Not ideal at 1M TPS.

Ask AI

### Distributed transaction using Try-Confirm/Cancel (TCC)

TCC is a compensation-style protocol: Try reserves resources, Confirm applies, Cancel rolls back. It's database-agnostic and supports parallel execution of tries.

Phases:

|Phase|Operation|A|C|
|---|---|---|---|
|1 Try|Reserve/reduce|Balance change: -$1|Do nothing|
|2 Confirm|Apply credit|Do nothing|Balance change: +$1|

Try, Confirm, Cancel sequences are handled by a coordinator. TCC is parallelizable (tries can run in parallel) and suitable where low latency is important.

Ask AI

### TCC Failure modes

The coordinator may die mid-flight. We must persist phase status in durable phase-status tables (atomically) so the coordinator or a recovery process can resume or roll back.

![phase-status-tables](https://nextleet.com/images/phase-status-tables.png)

Out-of-order execution is possible (cancel arrives before try). Use flags and idempotency checks in phase-status to detect and handle such cases. Always perform deductions before additions to avoid users spending intermediary (reserved) funds.

Ask AI

### Distributed transaction using Saga

Sagas sequence local transactions; on failure, compensating transactions undo previous steps. Two coordination styles: choreography (event-driven) or orchestration (central coordinator). For a wallet, orchestration is easier to reason about.

![saga](https://nextleet.com/images/saga.png)

Comparison (TCC vs Saga): TCC is parallelizable and may offer lower latency; Saga is linear but simpler to implement with compensation logic. Both expose brief inconsistent states which must be handled.

Ask AI

### Event sourcing

Event sourcing stores an immutable log of events (facts) so the entire state can be reconstructed by replaying events. This supports auditability, reproducibility, and easier debugging.

Key concepts:

- **Command** - intent (transfer $1). Commands can fail and may produce events.
- **Event** - a fact that happened (A debited $1, B credited $1). Events are immutable.
- **State** - derived from applying events (balances).
- **State machine** - a deterministic processor that validates commands and emits events.

![event-sourcing](https://nextleet.com/images/event-sourcing.png)

Flow: commands are placed in a FIFO queue (Kafka) to ensure global order; a state machine consumes commands, validates, and emits events; events are appended to the immutable event log and applied to the state.

Advantages: replayable history, deterministic state reconstruction, ability to run different code versions against the same event stream for verification.

![cqrs-architecture](https://nextleet.com/images/cqrs-architecture.png)

Use CQRS: separate write (event sourcing) from read (materialized views) to optimize queries and scalability.

Ask AI

## Step 3 - Design Deep Dive

We now examine performance optimizations required to reach 1M TPS, durability and replication strategies, and sharding with coordinated transactions.

Ask AI

### High-performance event sourcing

Optimizations to get extreme TPS:

- Append commands/events to a local disk instead of a remote broker (reduce network overhead).
- Use OS-backed memory mapping (mmap) to keep recent appends in cache for fast access.
- Store state locally using RocksDB (LSM-tree optimized for writes) or SQLite for compact local state.
- Periodically snapshot state to avoid replaying the entire history for recovery.

![mmap-optimization](https://nextleet.com/images/mmap-optimization.png)

![rocks-db-approach](https://nextleet.com/images/rocks-db-approach.png)

![snapshot-approach](https://nextleet.com/images/snapshot-approach.png)

Ask AI

### Reliable high-performance event sourcing

Local-first storage is fast but stateful - we must replicate event logs for durability while preserving order. Use consensus (Raft) to replicate the event log across a Raft group.

![raft-replication](https://nextleet.com/images/raft-replication.png)

Raft ensures a leader appends events and followers replicate in the same order. As long as a majority is available, the log is durable and ordered.

Ask AI

### Distributed event sourcing

Single Raft group capacity is limited. To scale, shard accounts across many Raft groups (partitions). Each partition is a Raft group with its own leader and followers.

![sharded-raft-groups](https://nextleet.com/images/sharded-raft-groups.png)

Cross-partition transfers require distributed coordination (Saga or TCC). To provide a smooth client UX, combine a reverse proxy + CQRS read machines that can push results back once materialized.

![reverse-proxy](https://nextleet.com/images/reverse-proxy.png)

![push-state-machines](https://nextleet.com/images/push-state-machines.png)

Example lifecycle: a client sends a request to the Saga coordinator -> the coordinator determines partitions -> sends commands to partition leaders -> the leaders convert commands to events and replicate via Raft -> read-side materializers update views and push responses -> the coordinator continues to the next steps and finally responds to the client.

Ask AI

## Step 4 - Wrap Up

Design evolution summary:

1. Started with an in-memory Redis approach - scalable but not durable.
2. Considered relational DBs with distributed transactions (2PC) - correct but heavy and not performant for 1M TPS.
3. Explored TCC and Saga for distributed transactions with compensating actions.
4. Adopted event sourcing for auditability, reproducibility and to simplify recovery.
5. Optimized by storing logs locally with mmap and RocksDB, snapshotting state, and using Raft for replication.
6. Scaled via sharded Raft groups and orchestrated cross-shard transactions (Saga/TCC) with CQRS and a reverse-proxy for better UX.

This architecture balances performance, durability, and correctness. Event sourcing + Raft + sharding + careful transaction orchestration (TCC or Saga) is the recommended direction for building a 1M TPS, auditable, and highly available digital wallet.