Google Drive is a cloud file storage product that lets users store documents, videos, and arbitrary files in the cloud. Files are accessible from any device and can be shared with other people. This design focuses on uploading/downloading files, syncing across devices, revision history, sharing, and notifications.

![Storage example](https://nextleet.com/images/storage-example.png)

Ask AI

## Step 1 - Understand the problem and establish scope

Clarifying requirements up front avoids wrong assumptions. Example Q&A used to scope this design:

- **Q:** What are the most important features?
- **A:** Upload/download files, file sync, and notifications.
- **Q:** Mobile or web?
- **A:** Both mobile and web.
- **Q:** Supported file formats?
- **A:** Any file type.
- **Q:** Files encrypted?
- **A:** Yes - files at rest must be encrypted.
- **Q:** File size limit?
- **A:** Files must be ≤ 10 GB.
- **Q:** How many users?
- **A:** 10 million DAU (example).

Primary features to design for:

- Add/upload files (large-file support and resumable uploads).
- Download files and efficient sync across devices.
- File revision history and restore.
- File sharing (links, permissions).
- Notifications on edits/deletes/shares.

Non-functional requirements: reliability (no data loss), fast sync, low bandwidth/battery usage on clients, scalability, and high availability.

Ask AI

## Back-of-the-envelope estimation

Estimate scale to guide capacity planning:

- Users: 50M signups, 10M DAU.
- Free allocation (example): 10 GB/user → 50M × 10 GB = 500 PB total provisioned.
- Uploads: 10M DAU × 2 files/day × average 0.5 MB = ~10M × 2 × 0.5 MB = 10 TB/day (approx). (This is a simplified example.)
- Upload API baseline QPS: 10M × 2 / (24 × 3600) ≈ 240 QPS; peak ~480 QPS.

Ask AI

## Step 2 - High-level design

We start from a single-server model and evolve to a distributed system as scale grows. Core components:

- Web/API servers - handle authentication, metadata, drive operations and generate upload/download URLs.
- Block servers (or upload proxies) - handle chunking, delta-sync, compression and encryption before persisting to blob storage.
- Blob (object) storage - durable storage (e.g., S3 or equivalent). Replication across regions for availability and durability.
- Metadata database - relational database for strong consistency (file metadata, permissions, revisions).
- Metadata cache - small hot cache for frequently accessed metadata.
- Notification service - pub/sub system to push change notifications to clients (or queue them for offline devices).
- Cold storage - cheaper tier for infrequently accessed historical revisions.

Example APIs:

```
POST https://api.example.com/files/upload?uploadType=resumable

// Use resumable upload flow: request an upload URL, then PUT multipart chunks to that URL. This supports resume on failure.
```

```
GET https://api.example.com/files/download

Payload: { "path": "/recipes/soup/best_soup.txt" }
```

```
GET https://api.example.com/files/list_revisions?path=/recipes/soup/best_soup.txt&limit=10
```

![Storage example](https://nextleet.com/images/storage-example.png)

Ask AI

### Scale out: storage, load balancing, metadata DB

When a single server hits capacity, scale by:

- **Use object storage (S3-like):** durability, region replication, and lifecycle policies simplify durability and cold storage.
- **Load balancers + web server fleet:** distribute API traffic across stateless web servers.
- **Metadata DB:** move metadata to a replicated, sharded relational DB for strong consistency and ACID guarantees for operations such as permission changes and revision commits.

![Cloud object storage example](https://nextleet.com/images/amazon-s3.png)

Ask AI

### Sync conflicts and versioning

Conflicts are inevitable when multiple clients modify the same file concurrently. A pragmatic approach:

- **First-writer-wins (fast path):** Allow the first write to commit; later conflicting writes create a new version (conflict copy) for user review.
- **Conflict detection:** Clients include a version or ETag; server compares and either accepts the change or creates a conflict revision.
- **User merge tooling:** Present both versions to the user (or attempt an automatic merge for text files) and let them choose which to keep or how to combine.

![Sync conflict example](https://nextleet.com/images/sync-conflict-example.png)

Ask AI

### High-level architecture diagram

At scale the system looks like:

- Clients communicate with API servers behind load balancers.
- API servers interact with metadata DB, metadata cache, block servers and blob storage.
- Notification service sends updates to clients (or stores offline events for later delivery).
- Cold storage and deduplication layers run offline or asynchronously.

![Updated simple design](https://nextleet.com/images/updated-simple-design.png)

Ask AI

### Step 3 - Deep dive: block servers

Block servers are responsible for efficient transfer and storage of file content. Key responsibilities:

- **Chunking:** Split large files into fixed-size blocks (e.g., 4 MB) for resumable uploads.
- **Delta sync:** When a file is edited, compute changed blocks and upload only deltas instead of entire file.
- **Compression:** Apply compression appropriate to content type (gzip for text, avoid for already-compressed binary formats).
- **Encryption:** Encrypt block payloads before writing to object storage (server-side encryption or client-side depending on policy).
- **Hashing:** Compute block hashes to enable deduplication.

![Block servers deep dive](https://nextleet.com/images/block-servers-deep-dive.png)

![Delta sync example](https://nextleet.com/images/delta-sync.png)

Ask AI

### Strong consistency requirements

Drive is expected to be strongly consistent - users must not see divergent versions. To achieve this:

- **Use transactional metadata store:** A relational DB (with ACID semantics) for metadata and version records ensures correct sequencing of operations.
- **Careful cache invalidation:** Invalidate or update metadata caches immediately on writes to avoid serving stale metadata.
- **Atomic commits:** Commit metadata (version record) only after content blocks are durably persisted; use two-phase commit patterns or write-ahead logs if necessary.

Ask AI

### Metadata database and schema

Metadata stores user info, device info, file entries, versions, and block references. Example high-level tables:

- **User** (user_id, name, email, storage_quota).
- **Device** (device_id, user_id, push_id).
- **File** (file_id, owner_id, path, current_version_id, permissions).
- **FileVersion** (version_id, file_id, created_at, author_device, immutable metadata).
- **Block** (block_hash, storage_location, size, ref_count).

![Metadata DB schema](https://nextleet.com/images/metadata-db-deep-dive.png)

Ask AI

### Upload flow (detailed)

The upload process typically has two parallel threads - metadata update and block/content upload:

1. **Client requests upload session:** API issues a resumable upload URL (presigned) and registers metadata with status "pending".
2. **Client uploads blocks:** Client PUTs chunks to the presigned URL; each successful chunk persists to blob storage and returns an acknowledgement.
3. **Block server processing:** Block servers may compress, encrypt, and compute hashes for each chunk before storing.
4. **Completion callback:** On final chunk upload, client or block server notifies API; server verifies block hashes and commits a new file version atomically in the metadata DB.
5. **Notifications:** Notification service publishes the change so other devices can sync.

![Upload flow](https://nextleet.com/images/upload-flow.png)

Ask AI

### Download and sync flow

When a change happens elsewhere, clients must be notified and reconcile local state:

1. **Notification:** Notification service alerts other client instances of the file change (push or queued delivery for offline devices).
2. **Fetch metadata:** Client fetches the latest metadata from API servers to learn which blocks changed and the new version id.
3. **Download blocks:** Client requests changed blocks from block servers or object storage and reconstructs the file locally.
4. **Local apply:** Client applies changes and updates local revision pointers. If conflict detected, client follows conflict resolution strategy.

![Download flow](https://nextleet.com/images/download-flow.png)

Ask AI

### Notification service (long polling vs WebSockets)

Clients must be notified about remote changes. Two common approaches:

- **Long polling:** Client sends a request that the server holds until an event or timeout occurs. Simpler to scale and works well for infrequent notifications.
- **WebSockets:** Persistent bidirectional socket for realtime notifications. More resource intensive but lower latency and bi-directional.

For Drive-like notification patterns (infrequent, server→client updates), long polling is a practical choice (Dropbox uses it). WebSockets are appropriate if you require very low-latency two-way interactions.

Ask AI

### Save storage space: deduplication and lifecycle

To control storage usage and cost:

- **Block-level deduplication:** Use block hashes to store identical blocks only once and maintain ref counts.
- **Version retention policy:** Limit the number of historical versions or compact frequent edits (e.g., coalesce multiple small changes into a single revision) while still offering 'restore' functionality.
- **Cold storage:** Move infrequently accessed revisions to cheaper cold storage (e.g., Glacier) with longer retrieval time.

![Delta sync helps deduplication](https://nextleet.com/images/delta-sync.png)

Ask AI

### Failure handling and redundancy

Design for component failures and degrade gracefully:

- **Load balancer failure:** Use active-passive or multi-region DNS failover.
- **Block server failure:** Retry on another replica; use multiple block servers and replicated queues.
- **Blob storage region failure:** Replicate objects across multiple regions and failover reads to the healthy region.
- **API server failure:** Stateless servers behind a load balancer are replaced automatically.
- **Metadata DB failure:** Use replication and automated failover (promote read replica to master), and keep consistent backups.
- **Notification service:** Use replicated pub/sub brokers; clients reconnect to alternate endpoints as needed.
- **Offline queue:** Queues persisted and replicated ensure offline changes are not lost and can be replayed.

Ask AI

## Step 4 - Wrap up and alternatives

Summary of the designed Drive-like system:

- **Strong consistency:** Metadata backed by relational DB and careful cache invalidation.
- **Low bandwidth and fast sync:** Block-level delta sync and resumable chunked uploads.
- **Durability and availability:** Blob storage with region replication and failover.
- **Storage efficiency:** Deduplication, retention policies, and cold storage for old revisions.

Alternative approaches and trade-offs:

- **Client-side chunking/encryption:** Upload chunks directly to object storage from the client. Pros: lower server bandwidth and latency. Cons: more client complexity, security concerns, and inconsistent implementations across platforms.
- **Event-driven sync service:** Move online/offline and sync coordination into a dedicated service so that other products can reuse it; this increases reuse at cost of additional operational complexity.

When presenting this in an interview, state assumptions (quotas, average file sizes, acceptable latencies), explain trade-offs (strong consistency vs. availability, server-side vs client-side processing), and discuss operational concerns (monitoring, backups, capacity planning, testing for failover).