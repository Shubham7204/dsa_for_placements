In this chapter, we'll be designing an object storage service, similar to Amazon S3. Storage systems fall into three broad categories: Block storage, File storage, and Object storage.

Block storage are devices like HDDs and SSDs. These can be physically or network-attached to a server, which manages data at a low level.

File storage is built on top of block storage and provides a higher-level abstraction for managing files and folders.

Object storage sacrifices performance for high durability, vast scale, and low cost. It stores data as immutable objects in a flat structure, making it ideal for archival and backup. Most cloud providers offer this type of storage, such as Amazon S3 and Google GCS.

| Block Storage   | File Storage                     | Object Storage                          |                                |
| --------------- | -------------------------------- | --------------------------------------- | ------------------------------ |
| Mutable Content | Y                                | Y                                       | N (has object versioning)      |
| Cost            | High                             | Medium to high                          | Low                            |
| Performance     | Medium to high, very high        | Medium to high                          | Low to medium                  |
| Consistency     | Strong consistency               | Strong consistency                      | Strong consistency [5]         |
| Data access     | SAS/iSCSI/FC                     | Standard file access, CIFS/SMB, and NFS | RESTful API                    |
| Scalability     | Medium scalability               | High scalability                        | Vast scalability               |
| Good for        | Virtual machines (VM), databases | General-purpose file system access      | Binary data, unstructured data |

Some terminology related to object storage:

- **Bucket** - a logical container for objects. Its name is globally unique.
- **Object** - an individual piece of data, stored in a bucket, that contains both data and metadata.
- **Versioning** - a feature that keeps multiple variants of an object in the same bucket.
- **URI** - each resource is uniquely identified by a Uniform Resource Identifier.
- **SLA** - a contract between a service provider and a client.

Amazon S3 Standard-Infrequent Access storage class SLAs include: Durability of 99.999999999% across multiple Availability Zones, resilience to a full Availability Zone destruction, and 99.9% availability.

Ask AI

## Step 1 - Understand the Problem and Establish Design Scope

- **C:** Which features should be included?
- **I:** Bucket creation, Object upload/download, versioning, Listing objects in a bucket.
- **C:** What is the typical data size?
- **I:** We need to store both massive objects and small objects efficiently.
- **C:** How much data do we store in a year?
- **I:** 100 petabytes.
- **C:** Can we assume 6 nines of data durability (99.9999%) and service availability of 4 nines (99.99%)?
- **I:** Yes, sounds reasonable.

Ask AI

## Non-functional requirements

- 100 PB of data.
- 6 nines of data durability.
- 4 nines of service availability.
- Storage efficiency. Reduce storage cost while maintaining high reliability and performance.

Ask AI

## Back-of-the-envelope estimation

Object storage is likely to have bottlenecks in disk capacity or IO per second (IOPS).

Assumptions:

- 20% small (less than 1mb), 60% mid-size (1-64mb), and 20% large objects (greater than 64mb).
- One hard disk (SATA, 7200rpm) is capable of 100-150 random seeks per second (100-150 IOPS).

Given these assumptions, we can estimate the total number of objects the system can persist.

- Let's use median size per object type: 0.5mb for small, 32mb for medium, 200mb for large.
- With 100 PB of storage ($10^{11}$ MB) and a 40% storage usage, this results in ~0.68 billion objects.
- Assuming metadata is 1kb, then we need ~0.68 TB space to store metadata info.

Ask AI

## Step 2 - Propose High-Level Design and Get Buy-In

Let's explore some interesting properties of object storage before diving into the design:

- **Object immutability** - objects are immutable. They can be deleted or replaced, but not updated.
- **Key-value store** - an object URI is its key and its contents can be accessed via an HTTP call.
- **Write once, read many times** - the data access pattern is writing once and reading many times. According to some research, 95% of operations are reads.
- **Supports both small and large objects.**

The design philosophy of object storage is similar to UNIX, where a file's metadata is stored in an inode and its data is stored in different disk locations. When accessing a file, its metadata is fetched first, followed by its contents.

![object-store-vs-unix](https://nextleet.com/images/object-store-vs-unix.png)

By separating metadata from file contents, we can scale the different stores independently.

![bucket-and-object](https://nextleet.com/images/bucket-and-object.png)

### High-level design

![high-level-design](https://nextleet.com/images/high-level-design.png)

- **Load balancer** - distributes API requests across service replicas.
- **API service** - a stateless server that orchestrates calls to the metadata store, data store, and IAM service.
- **Identity and access management (IAM)** - a central place for authentication, authorization, and access control.
- **Data store** - stores and retrieves actual data. Operations are based on an object's ID (UUID).
- **Metadata store** - stores object metadata.

### Uploading an object

![uploading-object](https://nextleet.com/images/uploading-object.png)

1. First, a bucket is created via an HTTP PUT request. The API service verifies the user's authorization with IAM and then calls the metadata store to create a bucket entry.
2. An HTTP PUT is sent to create an object. The API service verifies user permissions.
3. The object payload is sent to the data store, which persists it and returns a UUID.
4. The API service calls the metadata store to create a new entry with the object's metadata, including the UUID.

### Downloading an object

Although buckets have no directory hierarchy, we can create a logical one by using prefixes in object names. For example, 'images/photo.jpg'.

![download-object](https://nextleet.com/images/download-object.png)

1. A client sends an HTTP GET request to the load balancer for an object.
2. The API service queries IAM to verify read permissions.
3. The object's UUID is retrieved from the metadata store.
4. The object payload is retrieved from the data store based on the UUID and returned to the client.

Ask AI

## Step 3 - Design Deep Dive

Let's deep dive into the data store, metadata store, and optimizations.

### Data store

The API service interacts with the data store, which consists of several components.

![data-store-main-components](https://nextleet.com/images/data-store-main-components.png)

- **Data routing service** - a stateless service that routes API requests to the appropriate data nodes. It queries a placement service to find the best node for storing/retrieving data.
- **Placement service** - a critical service that maintains a virtual cluster map of data nodes. It uses a consensus algorithm like Raft to ensure consistency and sends heartbeats to data nodes to check their health.
- **Data nodes** - these store the actual object data and send heartbeats to the placement service with information about their disk drives and storage usage.

#### Data persistence flow

![data-persistence-flow](https://nextleet.com/images/data-persistence-flow.png)

1. The API service forwards the object data to the data store.
2. The data routing service sends the data to the primary data node.
3. The primary data node saves the data and replicates it to two secondary data nodes.
4. After successful replication, the primary node returns a UUID to the API service.

This flow favors strong consistency over lower latency. The replication group is deterministically chosen using consistent hashing based on the object's UUID.

#### How data is organized

Storing each object as a separate file is inefficient due to wasted disk space and inode limits. A better approach is to merge many small files into larger ones using a **write-ahead log (WAL)**. To avoid lock contention, these files can be confined to specific CPU cores.

![wal-optimization](https://nextleet.com/images/wal-optimization.png)

#### Object lookup

To locate objects within these larger files, a table is needed on each data node to map `object_id` to its filename, offset, and size. A lightweight, file-based relational database like SQLite is a good option for this.

#### Durability

To achieve 6 nines of durability, we must replicate data across different failure domains (e.g., cross-rack, cross-datacenter). A typical HDD failure rate of 0.81% means three replicas are needed for 6 nines of durability.

As an alternative to replication, **erasure coding** can be used to reduce storage costs. It uses parity bits to reconstruct lost data. An 8+4 scheme, for example, allows the loss of any 4 data pieces to be recovered from the remaining 8. Erasure coding is more durable and cost-effective but has higher latency and is more complex to implement.

![erasure-coding](https://nextleet.com/images/erasure-coding.png)

#### Correctness verification

To detect data corruption, we can use **checksums**. A checksum is a hash of the file's contents used to verify its integrity. We'll store checksums for both files and individual objects.

### Metadata data model

The metadata store holds bucket and object metadata. The object table is likely to be large and needs to be sharded. Sharding by `hash(bucket_name, object_name)` is a good choice for fast lookups. However, this makes listing objects with a common prefix slow, as it requires querying all shards.

To optimize object listing, we can create a denormalized table sharded by bucket ID. This isolates the query to a single database instance.

#### Object versioning

Versioning is implemented by adding an `object_version` column, often a TIMEUUID, to the object's record. A new version creates a new object ID. Deleting an object creates a new version with a special marker, causing a 404 response on future queries.

#### Optimizing uploads of large files

For large files, we use **multipart uploads**. A file is split into chunks that are uploaded independently. The service reassembles the object from these chunks and returns a success response. A garbage collector is used to clean up old or failed uploads.

#### Garbage collection

The garbage collector reclaims unused storage space from lazy deletions, orphan data from failed uploads, or corrupted data. This process often involves **compaction**, where valid objects are copied to a new file and old files are deleted.

Ask AI

## Step 4 - Wrap Up

Summary of what we covered:

- The design of an S3-like object storage service.
- A comparison between object, block, and file storage.
- The design for uploading, downloading, listing, and versioning objects.
- A deep dive into the design of the data and metadata stores, including replication, erasure coding, multipart uploads, and sharding.